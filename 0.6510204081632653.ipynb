{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import sklearn\n",
    "print (sklearn.__version__)\n",
    "from scipy import stats\n",
    "from scipy.stats import  norm,boxcox_normmax\n",
    "from scipy.stats import skew\n",
    "from sklearn.model_selection import cross_validate\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LassoCV,ElasticNetCV,RidgeCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm\n",
    "from sklearn.model_selection import cross_val_score,cross_val_score\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV    \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import  make_scorer\n",
    "#from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.preprocessing import Normalizer,LabelEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import warnings\n",
    "#import shap\n",
    "#shap.initjs()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def onehot_enconde(de_in,onehot_list):\n",
    "    de_out = de_in\n",
    "    for feature in onehot_list:\n",
    "        if feature in de_in.columns:\n",
    "            onehot = pd.get_dummies(de_in[feature],prefix=feature)\n",
    "            de_out = de_out.join(onehot)\n",
    "            de_out = de_out.drop(columns = feature)\n",
    "            print (de_out.shape)\n",
    "    return de_out\n",
    "\n",
    "\n",
    "def knn_income(all_data):\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "    data_train = pd.read_csv(\"train.csv\")\n",
    "    True_index = data_train['village_income_median'].notnull()\n",
    "    data_train = data_train [True_index] \n",
    "\n",
    "    y_train = data_train['village_income_median']\n",
    "    x_train = data_train[['lat','lon']]\n",
    "    model = KNeighborsRegressor(n_neighbors=10, algorithm='brute',weights = 'distance' )\n",
    "    model.fit(x_train, y_train) \n",
    "    return model\n",
    "\n",
    "#fill miss value // input:tained model , data_all // return fiexed data_all\n",
    "def fill_miss_value_by_knn(model,data_all):\n",
    "    False_index =  data_all['village_income_median'].isnull()\n",
    "    x_test = data_all[False_index]\n",
    "    x_test = x_test[['lat','lon']]\n",
    "    if x_test.shape[0]!=0:\n",
    "        pred = model.predict(x_test)\n",
    "\n",
    "        income_serise = data_all['village_income_median']\n",
    "        False_index = income_serise.isnull()\n",
    "        income_serise [False_index] = pred\n",
    "        data_all['village_income_median'] = income_serise\n",
    "    return data_all\n",
    "def dynamic_selection(number,x_train,y_train):\n",
    "    onehot_list = ['building_use','building_type','building_material',\n",
    "                   'town','city','parking_way','village']  \n",
    "\n",
    "    model = lightgbm.LGBMRegressor(objective='regression',num_leaves=30,\n",
    "                                   learning_rate=0.1, n_estimators=2000, max_depth=5, bagging_fraction = 0.77,feature_fraction = 0.81,\n",
    "                                   min_child_samples = 19, min_child_weight = 0.001,\n",
    "                                   reg_alpha = 0.2, reg_lambda = 0.9,\n",
    "                                   min_data_in_leaf = 16 , min_split_gain = 0,importance_type='gain',n_job=-1)\n",
    "\n",
    "    c = [c for c  in x_train.columns if  c in onehot_list ]\n",
    "    model.fit(x_train,y_train,categorical_feature = c)\n",
    "    \n",
    "    index = np.argsort((model.feature_importances_)*-1)\n",
    "    index = index[0:number]\n",
    "    cols = list(x_train.columns)\n",
    "    selected_features = [cols[i] for i in index ]\n",
    "    \n",
    "    \n",
    "          \n",
    "    return selected_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def my_custom_loss_func(y_pred, y_true):\n",
    "    y_true = np.expm1(y_true)\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    number_of_data = len(y_true)\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    rae = diff/y_true\n",
    "    z = rae[rae<=0.1]\n",
    "    hit_ratio = len(z)/number_of_data\n",
    "    return hit_ratio\n",
    "\n",
    "def my_custom_loss_func2(model,x, y_true):\n",
    "    y_pred = model.predict(x)\n",
    "    y_pred = y_pred #+ x.median_price\n",
    "    y_true = y_true #+ x.median_price\n",
    "    number_of_data = len(y_true)\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    rae = diff/y_true\n",
    "    z = rae[rae<=0.1]\n",
    "    hit_ratio = len(z)/number_of_data\n",
    "    return hit_ratio\n",
    "def fix_parking_area(x_train,x_test):\n",
    "    x_train = x_train.fillna(0)\n",
    "    x_test = x_test.fillna(0)\n",
    "    x_fit = x_train[data_train.parking_area>0]\n",
    "    y_fit = x_fit.parking_area\n",
    "    x_fit= x_fit.drop(columns =['total_price','building_id','parking_area'])\n",
    "\n",
    "    model = lightgbm.LGBMRegressor()\n",
    "    model.set_params(objective = 'regression', \n",
    "                     learning_rate = 0.1,\n",
    "                     n_estimators = 1000,\n",
    "                     num_leaves = 100,\n",
    "                     max_bin = 50,\n",
    "                     bagging_fraction = 1,\n",
    "                     bagging_freq = 5,\n",
    "                     feature_fraction = 1,\n",
    "                     num_threads = -1)   \n",
    "    y_fit = np.log1p(y_fit)\n",
    "    model.fit(x_fit,y_fit)\n",
    "    pred = model.predict(x_fit)\n",
    "    pred = np.expm1(pred)\n",
    "    y_fit = np.expm1(y_fit)\n",
    "    train_loss = np.sum((np.abs(pred-(y_fit))))/len(y_fit)\n",
    "    print (train_loss)\n",
    "    \n",
    "    train_target_index = (x_train.parking_price>0) & (x_train.parking_area==0)\n",
    "    test_target_index = (x_test.parking_price>0) & (x_test.parking_area==0)\n",
    "    temp = x_train[train_target_index].drop(columns =['total_price','building_id','parking_area'])\n",
    "    pred = model.predict(temp)\n",
    "    pred = np.expm1(pred)\n",
    "    pred[pred<1]=1\n",
    "    x_train['parking_area'][train_target_index] = pred\n",
    "    \n",
    "    temp = x_test[test_target_index].drop(columns =['total_price','building_id','parking_area'])\n",
    "    pred = model.predict(temp)\n",
    "    pred = np.expm1(pred)\n",
    "    pred[pred<0]=1\n",
    "    x_test['parking_area'][test_target_index] = pred\n",
    "    \n",
    "    return x_train,x_test\n",
    "\n",
    "def assign_area_by(k,data_kmeans,data_classifier):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    data_kmeans.total_price = data_kmeans.total_price/data_kmeans.building_area\n",
    "    scaled_feature = scaler.fit_transform(data_kmeans[['lat','lon','total_price']])\n",
    "    KMeans =  KMeans(n_clusters=k, random_state=0).fit(scaled_feature)\n",
    "    Ground_truth = KMeans.predict(scaled_feature)\n",
    "\n",
    "    neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "    scaler2 = MinMaxScaler()\n",
    "    scaler2.fit(data_classifier)\n",
    "    neigh.fit(scaler2.transform(data_classifier), Ground_truth)\n",
    "\n",
    "    return neigh,scaler2\n",
    "\n",
    "\n",
    "def fill_village(data_x,data_y):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    neigh = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "    neigh.fit(data_x, data_y)\n",
    "    \n",
    "\n",
    "    return neigh\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_city(i,data_train,data_test,building_type,eval_mode):\n",
    "    filter = data_train.building_type.isin(building_type)\n",
    "    data_train = data_train[filter]\n",
    "    filter = data_test.building_type.isin(building_type)\n",
    "    data_test=data_test[filter]\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    data_train.total_price = data_train.total_price/(data_train.building_area)\n",
    "\n",
    "\n",
    "    ID_test = pd.DataFrame({'building_id': data_test['building_id'],\n",
    "                            'building_area': data_test['building_area']})\n",
    "\n",
    "    all_data = pd.concat([data_train,data_test],keys=['train','test'])\n",
    "     \n",
    "        \n",
    "    \n",
    "    #x and y\n",
    "    y_train = all_data['total_price'].loc['train']\n",
    "    all_data = all_data.drop(columns='total_price')\n",
    " \n",
    "    \n",
    "    #remove unnecessary featrues\n",
    "    drop_list = ['building_id',\n",
    "                 'doc_rate','master_rate','bachelor_rate','jobschool_rate','elementary_rate','highschool_rate', 'junior_rate',\n",
    "                 'town_population','town_area','town_population_density',\n",
    "                 'divorce_rate','born_rate','death_rate','marriage_rate']\n",
    "    \n",
    "    all_data = all_data.drop(columns=drop_list)\n",
    "\n",
    "\n",
    "\n",
    "    all_data['lat'] = -all_data['lat']\n",
    "    all_data['parking_way'] = 2-all_data['parking_way']\n",
    "    all_data['LA_BA'] =  all_data['land_area']/(all_data['building_area']+1)\n",
    "    #all_data['LA*BA'] =  all_data['land_area']*(all_data['building_area']+1)\n",
    "    #all_data['LA+BA'] =  all_data['land_area']+(all_data['building_area']+1)\n",
    "      \n",
    "    all_data['PP_BA'] =  all_data['parking_price']/(all_data['building_area']+1)\n",
    "    all_data['PP_LA'] =  all_data['parking_price']/(all_data['land_area']+1)\n",
    "\n",
    "    all_data['F_TF'] =  all_data['txn_floor']/(all_data['total_floor'])\n",
    "    #all_data['F*TF'] =  all_data['txn_floor']*(all_data['total_floor'])\n",
    "       \n",
    "    \n",
    "    all_data['VI_10000/XIII_10000'] =  all_data['VI_10000']/(all_data['XIII_10000'])\n",
    "    all_data['X_500/XIII_10000'] =  all_data['X_500']/(all_data['XIII_10000'])\n",
    "    all_data['VI_10000/X_500'] =  all_data['VI_10000']/(all_data['X_500'])\n",
    "    all_data['II_MIN+I_MIN'] =  all_data['II_MIN']+(all_data['I_MIN'])\n",
    "    all_data['II_MIN+X_MIN'] =  all_data['II_MIN']+(all_data['X_MIN'])\n",
    "    all_data['I_MIN+X_MIN'] =  all_data['I_MIN']+(all_data['X_MIN'])\n",
    "    #all_data['total_MIN'] = all_data.I_MIN+all_data.II_MIN+all_data.III_MIN+all_data.IV_MIN+all_data.V_MIN+all_data.VII_MIN+all_data.VIII_MIN+all_data.XII_MIN\n",
    "    #all_data['I_MIN+III_MIN'] =  all_data['I_MIN']+(all_data['III_MIN'])\n",
    "    #all_data['I_MIN+IV_MIN'] =  all_data['I_MIN']+(all_data['IV_MIN'])\n",
    "    \n",
    "    #all_data['I_MIN+V_MIN'] =  all_data['I_MIN']+(all_data['V_MIN'])\n",
    "    \n",
    "    #all_data['I_MIN+VII_MIN'] =  all_data['I_MIN']+(all_data['VII_MIN'])\n",
    "    \n",
    "    #all_data['I_MIN+XII_MIN'] =  all_data['I_MIN']+(all_data['XII_MIN'])\n",
    "    #I_MIN II_MIN  III_MIN  IV_MIN  V_MIN VII_MIN VIII_MIN  XII_MIN\n",
    "\n",
    "    all_data['House_Age'] = all_data['txn_dt']-all_data['building_complete_dt']\n",
    "    all_data['House_Age_0.5y'] = all_data.House_Age.apply(lambda x:(int(x/182)))\n",
    "    all_data['House_Age_y'] = all_data.House_Age.apply(lambda x:(int(x/365)))\n",
    "    all_data['House_Age_3y'] = all_data.House_Age.apply(lambda x:(int(x/1095)))\n",
    "\n",
    "    all_data['txn_dt_0.5y'] =all_data.txn_dt.apply(lambda x:(int(x/182)))\n",
    "    all_data['txn_dt_y'] =all_data.txn_dt.apply(lambda x:(int(x/365)))\n",
    "    all_data['txn_dt_3y'] =all_data.txn_dt.apply(lambda x:(int(x/1095)))\n",
    "    \n",
    "    all_data['building_complete_dt_0.5y'] =all_data.building_complete_dt.apply(lambda x:(int(x/182)))\n",
    "    all_data['building_complete_dt_y'] =all_data.building_complete_dt.apply(lambda x:(int(x/365)))\n",
    "    all_data['building_complete_dt_3y'] =all_data.building_complete_dt.apply(lambda x:(int(x/1095)))\n",
    "\n",
    "    \n",
    "    selected_features = dynamic_selection(150,all_data.loc['train'],y_train)\n",
    "    print (selected_features)\n",
    "    all_data =all_data[selected_features]   \n",
    "    \n",
    "    #onehot encoding\n",
    "    onehot_list = ['building_use','building_type','building_material',\n",
    "                   'town','city','parking_way','village']  \n",
    "\n",
    "    onehot_list = ['building_type','town','city','parking_way']  \n",
    "    \n",
    "    cat_index = [all_data.columns.get_loc(c) for c in onehot_list if c in all_data]\n",
    "\n",
    "\n",
    "\n",
    "    x_train = all_data.loc['train']      \n",
    "    x_test = all_data.loc['test'] \n",
    "    \n",
    "   \n",
    "\n",
    "    print(\"train_x size:\",x_train.shape)\n",
    "    print(\"train_y size:\",y_train.shape)\n",
    "    print(\"testing size:\",x_test.shape)\n",
    "    print(\"ID_test size\",ID_test.shape)\n",
    "    # model \n",
    "\n",
    "    if eval_mode:\n",
    "\n",
    "        from sklearn.preprocessing import QuantileTransformer\n",
    "        lgb = lightgbm.LGBMRegressor(cat_column = cat_index)\n",
    "        lgb.set_params(  objective = 'fair', \n",
    "                         learning_rate = 0.01,\n",
    "                         n_estimators = 5000,\n",
    "                         num_leaves = 250,\n",
    "                         max_bin = 200,\n",
    "                         bagging_fraction = 0.8,\n",
    "                         bagging_freq = 5,\n",
    "                         feature_fraction = 0.4,\n",
    "                         num_threads = -1,\n",
    "                         min_child_samples = 10,)\n",
    "        #target_trans =  QuantileTransformer(n_quantiles=40000,output_distribution='normal')\n",
    "        #model = lgb\n",
    "        model = TransformedTargetRegressor(regressor = lgb, func=np.log1p, inverse_func=np.expm1)\n",
    "        #model = TransformedTargetRegressor(regressor = lgb,\n",
    "        #                                 transformer =target_trans )\n",
    "\n",
    "    \n",
    "     \n",
    "    if not eval_mode:\n",
    "        from sklearn.preprocessing import QuantileTransformer\n",
    "        lgb = lightgbm.LGBMRegressor(cat_column = cat_index)\n",
    "        lgb.set_params(  objective = 'fair', \n",
    "                         learning_rate = 0.01,\n",
    "                         n_estimators = 8000,\n",
    "                         num_leaves = 250,\n",
    "                         max_bin = 200,\n",
    "                         bagging_fraction = 0.8,\n",
    "                         bagging_freq = 5,\n",
    "                         feature_fraction = 0.4,\n",
    "                         num_threads = -1,\n",
    "                         min_child_samples = 10,)\n",
    "        \n",
    "        #model = lgb\n",
    "        \n",
    "        #target_trans =  QuantileTransformer(n_quantiles=10000,output_distribution='normal')\n",
    "        #model = lgb\n",
    "        model = TransformedTargetRegressor(regressor = lgb, func=np.log1p, inverse_func=np.expm1)\n",
    "        #model = TransformedTargetRegressor(regressor = lgb,\n",
    "        #                                  transformer =target_trans )\n",
    "\n",
    "    \n",
    "    \n",
    "    eval_score=1\n",
    "    if eval_mode:\n",
    "        \n",
    "        cv_results = cross_validate(model, x_train, y_train,cv=6,\n",
    "                                    scoring = my_custom_loss_func2,\n",
    "                                    n_jobs = 1,\n",
    "                                    return_train_score = True,\n",
    "                                    verbose = 10 )\n",
    "        \n",
    "        \n",
    "\n",
    "        print ('train score:',np.median(cv_results['train_score']))\n",
    "        print ('test score:',np.median(cv_results['test_score']))\n",
    "        eval_score = np.median(cv_results['test_score'])\n",
    "  \n",
    "    if not eval_mode:\n",
    "        model.fit(x_train,y_train)\n",
    "        pred = model.predict(x_test)\n",
    "        ID_test['total_price'] = (pred)*(ID_test.building_area)\n",
    "        print ('train score:',my_custom_loss_func2(model,(x_train),y_train))      \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    return ID_test,ID_test.shape[0],eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "size = []\n",
    "score = []\n",
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test['total_price']=0\n",
    "data_train.parking_area = data_train.parking_area.fillna(0)\n",
    "#data_train.parking_price = data_train.parking_price.fillna(0)\n",
    "#data_train,data_test = fix_parking_area(data_train,data_test)\n",
    "#model =knn_income(data_train)\n",
    "#data_train = fill_miss_value_by_knn(model,data_train)\n",
    "#data_test = fill_miss_value_by_knn(model,data_test)\n",
    "    \n",
    "\n",
    "all_data = pd.concat([data_train,data_test],keys=['train','test'])\n",
    "\n",
    "gp_city = all_data.groupby(['city'])\n",
    "gp_result = gp_city.median().sort_values(by=['parking_price'])\n",
    "gp_result['label'] = np.arange(len(gp_result))\n",
    "all_data.city = [gp_result.loc[row[1]['city'],'label'] for row in all_data.iterrows() ]\n",
    "\n",
    "gp_town = all_data.groupby(['city','town'])\n",
    "gp_result = gp_town.median().sort_values(by=['parking_price'])\n",
    "gp_result['label'] = np.arange(len(gp_result))\n",
    "all_data.town = [gp_result.loc[(row[1]['city'],row[1]['town']),'label'] for row in all_data.iterrows() ]\n",
    "\n",
    "\n",
    "gp_village = all_data.groupby(['city','town','village'])\n",
    "gp_result = gp_village.median().sort_values(by=['parking_price'])\n",
    "gp_result['label'] = np.arange(len(gp_result))\n",
    "all_data.village = [gp_result.loc[(row[1]['city'],row[1]['town'],row[1]['village']),'label'] for row in all_data.iterrows() ]\n",
    "\n",
    "data_train = all_data.loc['train']\n",
    "data_test = all_data.loc['test']\n",
    "\n",
    "k=50  \n",
    "model ,scaler=  assign_area_by(k,data_train[['lat','lon','building_area','total_price']],data_train[['lat','lon']])\n",
    "data_train['k1'] = model.predict(scaler.transform(data_train[['lat','lon']]))\n",
    "data_test['k1'] =  model.predict(scaler.transform(data_test[['lat','lon']]))\n",
    "\n",
    "\n",
    "\n",
    "k=100  \n",
    "model ,scaler=  assign_area_by(k,data_train[['lat','lon','building_area','total_price']],data_train[['lat','lon']])\n",
    "data_train['k2'] = model.predict(scaler.transform(data_train[['lat','lon']]))\n",
    "data_test['k2'] =  model.predict(scaler.transform(data_test[['lat','lon']]))\n",
    "\n",
    "\n",
    "k=400  \n",
    "model ,scaler=  assign_area_by(k,data_train[['lat','lon','building_area','total_price']],data_train[['lat','lon']])\n",
    "data_train['k3'] = model.predict(scaler.transform(data_train[['lat','lon']]))\n",
    "data_test['k3'] =  model.predict(scaler.transform(data_test[['lat','lon']]))\n",
    "\n",
    "\n",
    "all_data = pd.concat([data_train,data_test],keys=['train','test'])\n",
    "\n",
    "gp_city = all_data.groupby(['k1'])\n",
    "gp_result = gp_city.median().sort_values(by=['parking_price'])\n",
    "gp_result['label'] = np.arange(len(gp_result))\n",
    "all_data.k1 = [gp_result.loc[row[1]['k1'],'label'] for row in all_data.iterrows() ]\n",
    "\n",
    "\n",
    "gp_city = all_data.groupby(['k2'])\n",
    "gp_result = gp_city.median().sort_values(by=['parking_price'])\n",
    "gp_result['label'] = np.arange(len(gp_result))\n",
    "all_data.k2 = [gp_result.loc[row[1]['k2'],'label'] for row in all_data.iterrows() ]\n",
    "\n",
    "\n",
    "gp_city = all_data.groupby(['k3'])\n",
    "gp_result = gp_city.median().sort_values(by=['parking_price'])\n",
    "gp_result['label'] = np.arange(len(gp_result))\n",
    "all_data.k3 = [gp_result.loc[row[1]['k3'],'label'] for row in all_data.iterrows() ]\n",
    "\n",
    "\n",
    "data_train = all_data.loc['train']\n",
    "data_test = all_data.loc['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  : [0, 1, 2, 3]\n",
      "['town', 'k1', 'village', 'k2', 'parking_price', 'XIII_10000', 'building_area', 'XIII_5000', 'building_complete_dt', 'txn_floor', 'txn_dt', 'F_TF', 'k3', 'building_complete_dt_0.5y', 'village_income_median', 'total_floor', 'VI_10000', 'V_MIN', 'X_500', 'land_area', 'VII_500', 'House_Age', 'building_type', 'city', 'II_1000', 'II_MIN', 'IV_5000', 'LA_BA', 'V_500', 'IX_10000', 'I_MIN+X_MIN', 'building_complete_dt_y', 'PP_LA', 'PP_BA', 'V_250', 'X_250', 'VII_250', 'XI_1000', 'II_MIN+I_MIN', 'XIV_500', 'VIII_1000', 'txn_dt_0.5y', 'IX_1000', 'II_MIN+X_MIN', 'parking_area', 'XII_MIN', 'XIII_1000', 'building_complete_dt_3y', 'X_5000', 'I_MIN', 'II_500', 'XIII_MIN', 'V_1000', 'X_500/XIII_10000', 'VIII_500', 'IX_500', 'XIII_500', 'IX_250', 'VIII_MIN', 'XIV_1000', 'VII_MIN', 'House_Age_0.5y', 'IV_1000', 'VII_1000', 'VI_10000/X_500', 'XIV_MIN', 'X_MIN', 'XIV_5000', 'txn_dt_y', 'III_MIN', 'IX_MIN', 'III_500', 'parking_way', 'XII_250', 'VI_MIN', 'I_1000', 'XI_MIN', 'X_1000', 'II_100', 'X_10000', 'IV_MIN', 'III_1000', 'V_5000', 'XII_1000', 'VII_5000', 'II_250', 'X_100', 'XII_100', 'VIII_250', 'House_Age_y', 'VII_100', 'III_250', 'XI_5000', 'VI_10000/XIII_10000', 'III_100', 'XII_500', 'IX_5000', 'IV_250', 'I_500', 'XI_500', 'VI_1000', 'lon', 'building_use', 'XII_50', 'VI_5000', 'XIV_10000', 'V_50', 'N_50', 'I_5000', 'VII_50', 'V_100', 'VIII_100', 'XIV_250', 'V_10000', 'building_material', 'IV_10000', 'XII_5000', 'IX_100', 'II_5000', 'VII_10000', 'XI_250', 'VIII_5000', 'XIII_250', 'I_10000', 'III_5000', 'III_50', 'XIV_100', 'VIII_50', 'IV_500', 'lat', 'XIII_100', 'IX_50', 'III_10', 'VI_500', 'I_250', 'XIII_10', 'XI_10000', 'I_100', 'XI_100', 'III_10000', 'XIV_50', 'VI_250', 'XII_10', 'II_50', 'II_10000', 'XIII_50', 'XI_50', 'XII_10000', 'N_500', 'VIII_10000']\n",
      "train_x size: (44098, 150)\n",
      "train_y size: (44098,)\n",
      "testing size: (7361, 150)\n",
      "ID_test size (7361, 2)\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ , score=(train=0.997, test=0.654), total= 2.2min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ , score=(train=0.997, test=0.648), total= 2.2min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ , score=(train=0.997, test=0.654), total= 2.1min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  7.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ , score=(train=0.996, test=0.656), total= 2.2min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 10.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ , score=(train=0.997, test=0.639), total= 2.2min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ , score=(train=0.997, test=0.632), total= 2.2min\n",
      "train score: 0.9968434078334226\n",
      "test score: 0.6510204081632653\n",
      "training on  : [4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 15.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 15.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['land_area', 'LA_BA', 'village', 'building_complete_dt', 'House_Age', 'XIII_10000', 'XIV_10000', 'building_area', 'IX_MIN', 'X_1000', 'XIV_MIN', 'III_500', 'VI_MIN', 'XI_250', 'IV_5000', 'lon', 'XII_5000', 'town', 'VII_MIN', 'II_MIN+I_MIN', 'IX_5000', 'k2', 'VII_250', 'X_MIN', 'X_500/XIII_10000', 'city', 'V_250', 'XI_MIN', 'XI_10000', 'I_5000', 'XI_100', 'X_500', 'VI_10000/X_500', 'VII_50', 'IV_250', 'IV_10000', 'I_10000', 'XII_500', 'k1', 'II_5000', 'village_income_median', 'II_1000', 'III_10000', 'VI_5000', 'V_10000', 'XIII_5000', 'IV_MIN', 'VIII_5000', 'V_5000', 'VIII_500', 'k3', 'X_100', 'XII_MIN', 'X_10000', 'XIII_1000', 'XI_5000', 'VIII_1000', 'II_MIN+X_MIN', 'IX_250', 'IV_1000', 'V_1000', 'XII_10000', 'I_MIN', 'House_Age_0.5y', 'V_MIN', 'VI_1000', 'txn_dt', 'VII_10000', 'VIII_MIN', 'X_5000', 'VIII_100', 'VI_500', 'XII_1000', 'VII_1000', 'VI_10000', 'II_500', 'I_MIN+X_MIN', 'VII_500', 'I_250', 'XII_250', 'IX_1000', 'III_1000', 'total_floor', 'VIII_10', 'VII_10', 'III_MIN', 'building_complete_dt_0.5y', 'VII_5000', 'VIII_50', 'XI_500', 'N_500', 'XIV_5000', 'XIV_1000', 'V_500', 'House_Age_y', 'XIII_500', 'I_500', 'III_250', 'VII_100', 'lat', 'IX_50', 'XII_100', 'VIII_250', 'XI_1000', 'X_250', 'txn_dt_0.5y', 'II_250', 'XIV_250', 'II_MIN', 'XIII_MIN', 'N_50', 'V_100', 'XIV_500', 'III_5000', 'I_1000', 'building_use', 'VI_10000/XIII_10000', 'VIII_10000', 'building_complete_dt_y', 'II_10000', 'XIV_10', 'III_100', 'IX_10000', 'IX_500', 'XIV_100', 'XIV_50', 'XII_50', 'XIII_250', 'txn_dt_y', 'II_100', 'VI_50', 'XI_index_500', 'building_complete_dt_3y', 'III_50', 'IV_500', 'IV_10', 'IX_100', 'V_50', 'IV_50', 'X_50', 'II_50', 'XII_10', 'XIII_100', 'XIII_50', 'txn_dt_3y', 'N_1000', 'building_material', 'X_index_50', 'II_index_50', 'parking_price']\n",
      "train_x size: (15902, 150)\n",
      "train_y size: (15902,)\n",
      "testing size: (2639, 150)\n",
      "ID_test size (2639, 2)\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in [0]:#np.arange(-1,1,0.01):\n",
    "    result=[]\n",
    "    size = []\n",
    "    score = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for building_type in [[0,1,2,3],[4]]:\n",
    "        print (\"training on  :\",building_type)\n",
    "        answer,eval_size,eval_score = train_city(i,data_train,data_test,building_type,1)\n",
    "        size.append(eval_size)\n",
    "        score.append(eval_score)\n",
    "        result.append(answer)\n",
    "\n",
    "\n",
    "    total_hit_ratio = sum(np.array(size)/(sum(size))*score)\n",
    "    print ('total_hit_ratio',total_hit_ratio,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = pd.concat(result)\n",
    "answer = answer.sort_index()\n",
    "answer = answer.drop(columns = ['building_area'])\n",
    "answer.to_csv(\"out.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.6438095238095238"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
